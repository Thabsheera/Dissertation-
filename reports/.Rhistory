# perform hierarchical clustering
hc= hclust(d,method = "complete")
summary(hc)
## Plot the dendrogram:
plot(hc, cex=0.5, main="", sub="", xlab="")
plot(hc$height, xlab="Fusion number", ylab="Fusion height")
# transposing and computing the inter -observation correlation
r_ops=cor(t(gexpr))
# subtract each correlation from 1 to get distance matrix
d_cor=1-r_ops
d= as.dist(d_cor)
# perform hierarchical clustering
hc= hclust(d,method = "single")
#summary(hc)
## Plot the dendrogram:
plot(hc, cex=0.5, main="", sub="", xlab="")
plot(hc$height, xlab="Fusion number", ylab="Fusion height")
?cutree
# transposing and computing the inter -observation correlation
r_ops=cor(t(gexpr))
# subtract each correlation from 1 to get distance matrix
d_cor=1-r_ops
d= as.dist(d_cor)
# perform hierarchical clustering
hc= hclust(d,method = "complete")
summary(hc)
# transposing and computing the inter -observation correlation
r_ops=cor(t(gexpr))
# subtract each correlation from 1 to get distance matrix
d_cor=1-r_ops
d= as.dist(d_cor)
# perform hierarchical clustering
hc= hclust(d,method = "complete")
#summary(hc)
## Plot the dendrogram:
plot(hc, cex=0.5, main="", sub="", xlab="")
plot(hc$height, xlab="Fusion number", ylab="Fusion height")
tree_cut=cutree(hc,k=2)
print(tree_cut)
library(dplyr)
table(tree_cut)
# transposing and computing the inter -observation correlation
r_ops=cor(t(gexpr))
# subtract each correlation from 1 to get distance matrix
d_cor=1-r_ops
d= as.dist(d_cor)
# perform hierarchical clustering
hc= hclust(d,method = "average")
summary(hc)
# transposing and computing the inter -observation correlation
r_ops=cor(t(gexpr))
# subtract each correlation from 1 to get distance matrix
d_cor=1-r_ops
d= as.dist(d_cor)
# perform hierarchical clustering
hc= hclust(d,method = "average")
#summary(hc)
## Plot the dendrogram:
plot(hc, cex=0.5, main="", sub="", xlab="")
plot(hc$height, xlab="Fusion number", ylab="Fusion height")
# transposing and computing the inter -observation correlation
r_ops=cor(t(gexpr))
# subtract each correlation from 1 to get distance matrix
d_cor=1-r_ops
d= as.dist(d_cor)
# perform hierarchical clustering
hc= hclust(d,method = "average")
#summary(hc)
## Plot the dendrogram:
plot(hc, cex=0.5, main="", sub="", xlab="")
plot(hc$height, xlab="Fusion number", ylab="Fusion height")
tree_cut=cutree(hc,k=2)
print(tree_cut)
table(tree_cut)
# computing he euclidian distance matrix
d= dist(gexpr)
# perform hierarchical clustering
hc= hclust(d,method = "single")
#summary(hc)
## Plot the dendrogram:
plot(hc, cex=0.5, main="", sub="", xlab="")
plot(hc$height, xlab="Fusion number", ylab="Fusion height")
tree_cut=cutree(hc,k=2)
print(tree_cut)
table(tree_cut)
set.seed(1)
# setting the max value of k
kmax=10
# initializing a vector to store ssw for each k
SS_W = numeric(kmax)
# storing the k means output to a list
km = list()
# loop over k and extract ssw values
for(K in 1:kmax) {
km[[K]] = kmeans(gexpr, K, iter.max = 50, nstart = 20)
SS_W[K] = km[[K]]$tot.withinss
}
# Plot K against SS_W
plot(1:kmax, SS_W, type="l", xlab="K", ylab="SS_W")
sort(km[[3]]$cluster)
## Perform the PCA:
pca_nci = prcomp(x=gexpr)
## Plot the first PC against the second PC
plot(pca_nci$x[,1], pca_nci$x[,2], xlab="First PC", ylab="Second PC",
col=km[[4]]$cluster, pch=km[[4]]$cluster)
## Add labels
text(pca_nci$x[,1], pca_nci$x[,2], labels=rownames(gexpr), cex=0.5, pos=3,
col="darkgrey")
## Perform the PCA:
pca_nci = prcomp(x=gexpr)
## Plot the first PC against the second PC
plot(pca_nci$x[,1], pca_nci$x[,2], xlab="First PC", ylab="Second PC",
col=km[[2]]$cluster, pch=km[[2]]$cluster)
## Add labels
text(pca_nci$x[,1], pca_nci$x[,2], labels=rownames(gexpr), cex=0.5, pos=3,
col="darkgrey")
tree_cut=cutree(hc,h = 0.75)
print(tree_cut)
table(tree_cut)
tree_cut=cutree(hc,k = 2)
print(tree_cut)
table(tree_cut)
## Plot the dendrogram:
plot(hc, cex=0.5, main="", sub="", xlab="")
plot(hc$height, xlab="Fusion number", ylab="Fusion height")
# transposing and computing the inter -observation correlation
r_ops=cor(t(gexpr))
# subtract each correlation from 1 to get distance matrix
d_cor=1-r_ops
d= as.dist(d_cor)
# perform hierarchical clustering
hc= hclust(d,method = "single")
#summary(hc)
## Plot the dendrogram:
plot(hc, cex=0.5, main="", sub="", xlab="")
plot(hc$height, xlab="Fusion number", ylab="Fusion height")
tree_cut=cutree(hc,h=0.75)
print(tree_cut)
table(tree_cut)
tree_cut=cutree(hc,k=2)
print(tree_cut)
table(tree_cut)
sort(km[[2]]$cluster)
km[[2]]$centers
aggregate(gexpr, by=list(km[[4]]$cluster), mean)
aggregate(gexpr, by=list(km[[2]]$cluster), mean)
#Compare to overall means:
colMeans(gexpr)
View(gexpr)
## Perform the PCA:
pca_nci = prcomp(x=gexpr)
## Plot the first PC against the second PC
plot(pca_nci$x[,1], pca_nci$x[,2], xlab="First PC", ylab="Second PC",
col=km[[2]]$cluster, pch=km[[2]]$cluster)
## Add labels
text(pca_nci$x[,1], pca_nci$x[,2], labels=rownames(gexpr), cex=0.5, pos=3,
col="darkgrey")
## Perform the PCA:
pca_nci = prcomp(x=gexpr)
## Plot the first PC against the second PC
plot(pca_nci$x[,1], pca_nci$x[,2], xlab="First PC", ylab="Second PC",
col=km[[4]]$cluster, pch=km[[4]]$cluster)
## Add labels
text(pca_nci$x[,1], pca_nci$x[,2], labels=rownames(gexpr), cex=0.5, pos=3,
col="darkgrey")
# Load the nclSLR package
library(nclSLR)
## Load the data
data(diabetes)
## Store n and p:
(n = nrow(diabetes))
(p = ncol(diabetes) - 1)
apply(diabetes[,1:10],2,sd)
train<- diabetes[1:350,]
test<- diabetes[351:n,]
# Fit model using least squares
lsq_fit = lm(dis ~ ., data=train)
lsq_summary = summary(lsq_fit)
lsq_summary
# predicting on test data
yhat=predict(lsq_fit,test)
mean((test$dis - yhat) ^ 2)
test$dis
# predicting on test data
yhat=predict(lsq_fit,test)
mean((test$dis - yhat) ^ 2)
# fit model using least squares and with 6 predictors
lsq_fit_bss=lm(dis~sex+bmi+map+tc+ldl+ltg,data = train)
summary(lsq_fit_bss)
# predicting on test data set
yhat=predict(lsq_fit_bss,test)
mean((test$dis - yhat) ^ 2)
# predicting on test data set
yhat=predict(lsq_fit_bss,test)
mean((test$dis - yhat) ^ 2)
library(glmnet)
# finding the optimal value of tuning parameter using corss validation
grid = 10^seq(5, -3, length=100)
ridge_cv_fit = cv.glmnet(as.matrix(train[,1:10]),train$dis, alpha=0, standardize=FALSE,lambda = grid)
plot(ridge_cv_fit)
# finding the  lambda value corresponding to minimum
(lambda_min = ridge_cv_fit$lambda.min)
log(0.01353048)
# tuning parameter which was the minimum
(i = which(ridge_cv_fit$lambda == ridge_cv_fit$lambda.min))
# corresponding mse
ridge_cv_fit$cvm[i]
# using the lamda from the previous question and run ridge regression
ridge_fit = glmnet(as.matrix(train[,1:10]),train$dis, alpha=0, standardize=FALSE, lambda=lambda_min)
beta_hat=coef(ridge_fit)
(beta_hat=coef(ridge_fit))
yhat=predict(ridge_fit, newx=as.matrix(test[,1:10]), s=lambda_min)
mean((test$dis - yhat) ^ 2)
# list of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
# finding the optimal value of tuning parameter using corss validation
ridge_cv_fit = cv.glmnet(as.matrix(diabetes[,1:10]),diabetes$dis, alpha=0, standardize=FALSE,lambda = grid)
plot(ridge_cv_fit)
# finding the  lambda value corresponding to minimum
(lambda_min = ridge_cv_fit$lambda.min)
log(0.001)
# tuning parameter which was the minimum
(i = which(ridge_cv_fit$lambda == ridge_cv_fit$lambda.min))
# corresponding mse
ridge_cv_fit$cvm[i]
# list of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(as.matrix(diabetes[,1:10]),diabetes$dis, alpha=0, standardize=FALSE, lambda=grid)
beta_hat=coef(ridge_fit)
plot(ridge_fit, xvar="lambda", col=1:8, label=TRUE)
# running cross validation to get lambda
ridge_cv_fit = cv.glmnet(as.matrix(train[,1:10]),train$dis, alpha=0, standardize=FALSE, lambda=grid)
plot(ridge_cv_fit)
# finding the  lambda value
(lambda_min = ridge_cv_fit$lambda.min)
log(0.02364489)
# tuning parameter which was the minimum
(i = which(ridge_cv_fit$lambda == ridge_cv_fit$lambda.min))
# corresponding mse
ridge_cv_fit$cvm[i]
coef(ridge_fit, s=lambda_min)
# Fit model using least squares
lsq_fit_all = lm(dis ~ ., data=diabetes)
lsq_summary = summary(lsq_fit_all)
lsq_summary
?diabetes
coef(ridge_fit, s=lambda_min)
yhat=predict(lsq_fit,test)
mean((test$dis - yhat) ^ 2)
yhat=predict(lsq_fit_bss,test)
mean((test$dis - yhat) ^ 2)
yhat=predict(ridge_fit, newx=as.matrix(test[,1:10]), s=lambda_min)
mean((test$dis - yhat) ^ 2)
yhat=predict(ridge_fit, newx=as.matrix(test[,1:10]), s=lambda_min)
mean((test$dis - yhat) ^ 2)
yhat=predict(lsq_fit,test)
mean((test$dis - yhat) ^ 2)
yhat=predict(lsq_fit_bss,test)
mean((test$dis - yhat) ^ 2)
yhat=predict(ridge_fit, newx=as.matrix(test[,1:10]), s=lambda_min)
mean((test$dis - yhat) ^ 2)
View(gexpr)
km[[2]]$tot.withinss
knitr::opts_chunk$set(echo = TRUE)
## Load mlbench package
library(mlbench)
## Load the data
data(BreastCancer)
## Check size
dim(BreastCancer)
#head(Breast Cancer)
# converting all columns except class and id to integer
for (i in 2:10) {
BreastCancer[,i]<- as.integer(BreastCancer[,i])
}
# checking the type of class
typeof(BreastCancer$Class)
## Create new column of 0s and 1s for class
BreastCancer_new<- data.frame(BreastCancer[,-11],class=as.integer(BreastCancer$Class)-1)
# converting the class to factors before running models
BreastCancer_new$class<-as.factor(BreastCancer_new$class)
# checking the number of rows with NA
sum(is.na(BreastCancer))
# taking only the rows which has all values except NA
BreastCancer_new<- BreastCancer_new[complete.cases(BreastCancer_new),]
# plotting the paired correlation graph
library(GGally)
ggpairs(BreastCancer_new, columns = 2:11, ggplot2::aes(colour=as.character(class)))
# summary of table
summary<-summary(BreastCancer_new)
summary
# correlation
cor<-round(cor(BreastCancer_new[,2:10]),2)
cor
library(leaps)
# Perform forward stepwise selection:
fss = regsubsets(class ~ ., data=BreastCancer_new[,-1], method="forward", nvmax=9)
summary(fss)
# Perform backward stepwise selection:
bss = regsubsets(class ~ ., data=BreastCancer_new[,-1], method="backward", nvmax=10)
(bss_summary=summary(bss))
# Set up a 2x2 grid to show 4 plots at once
par(mfrow = c(2,2))
# plotting rss
plot(bss_summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "b")
# selecting the minimum value to highlight in the plot
rss_min = which.min(bss_summary$rss)
points(rss_min, bss_summary$rss[rss_min], col = "red", cex = 2, pch = 20)
# adjested r2
plot(bss_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "b")
adj_r2_max = which.max(bss_summary$adjr2)
points(adj_r2_max, bss_summary$adjr2[adj_r2_max], col ="red", cex = 2, pch = 20)
#cp
plot(bss_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "b")
cp_min = which.min(bss_summary$cp)
points(cp_min, bss_summary$cp[cp_min], col = "red", cex = 2, pch = 20)
#bic
plot(bss_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "b")
bic_min = which.min(bss_summary$bic)
points(bic_min, bss_summary$bic[bic_min], col = "red", cex = 2, pch = 20)
coef(bss,6)
## Obtain regression coefficients for this model
logreg_step_fit = glm(class ~ Cl.thickness+Cell.size+Cell.shape+Bare.nuclei+Bl.cromatin+Normal.nucleoli, data=BreastCancer_new, family="binomial")
summary(logreg_step_fit)
# Compute predicted probabilities:
phat = predict(logreg_step_fit, BreastCancer_new[,c(-1,-11)], type="response")
## Compute fitted values:
yhat = ifelse(phat > 0.5, 1, 0)
## Calculate confusion matrix:
(confusion = table(Observed=BreastCancer_new[,11], Predicted=yhat))
1 - mean(BreastCancer_new[,11]==yhat)
# pick out and scale the predictor variables
X1<- BreastCancer_new[,2:10]
X1<- scale(X1)
# pick the predictor variable
Y<- BreastCancer_new[,11]
# combining both
breastcancer_lr_data<- data.frame(X1,Y)
# Store n and p
n = nrow(breastcancer_lr_data)
p = ncol(breastcancer_lr_data) - 1
# Load the bestglm package
library(bestglm)
## Apply best subset selection
#aic
bss_fit_AIC = bestglm(breastcancer_lr_data, family=binomial, IC="AIC")
#BIC
bss_fit_BIC = bestglm(breastcancer_lr_data, family=binomial, IC="BIC")
# subsets
(AIC_subsets=bss_fit_AIC$Subsets)
(BIC_subsets=bss_fit_BIC$Subsets)
(best_AIC=bss_fit_AIC$ModelReport$Bestk)
(best_BIC=bss_fit_BIC$ModelReport$Bestk)
## Create multi-panel plotting device
par(mfrow=c(1,2))
## Produce plots, highlighting optimal value of k
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
pstar = 5
## Check which predictors are in the 1-predictor model
bss_fit_BIC$Subsets[pstar+1,]
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_BIC$Subsets[pstar+1, 2:(p+1)]))
breastcancer_lr_data_v1 = data.frame(X1[,indices], Y)
## Obtain regression coefficients for this model
logreg1_fit = glm(Y ~ ., data=breastcancer_lr_data_v1, family="binomial")
summary(logreg1_fit)
# Compute predicted probabilities:
phat = predict(logreg1_fit, breastcancer_lr_data_v1[,c(-6)], type="response")
## Compute fitted values:
yhat = ifelse(phat > 0.5, 1, 0)
1 - mean(breastcancer_lr_data_v1[,6]==yhat)
# Load the glmnet package
library(glmnet)
## Choose grid of values for the tuning parameter
grid = 10^seq(-5, 1, length.out=100)
## Fit a model with LASSO penalty for each value of the tuning parameter
lasso_fit = glmnet(X1, Y, family="binomial", alpha=1, standardize=FALSE, lambda=grid)
summary(lasso_fit)
# Examine the effect of the tuning parameter on the parameter estimates
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
lasso_cv_fit = cv.glmnet(X1, Y, family="binomial", alpha=1, standardize=FALSE, lambda=grid,
type.measure="class")
plot(lasso_cv_fit)
# Identify the optimal value for the tuning parameter
(lambda_lasso_min = lasso_cv_fit$lambda.min)
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min)
## Find the parameter estimates associated with optimal value of the tuning parameter
coef(lasso_fit, s=lambda_lasso_min)
# Compute predicted probabilities:
phat = predict(lasso_fit, X1, s=lambda_lasso_min, type="response")
## Compute fitted values:
yhat = ifelse(phat > 0.5, 1, 0)
## Calculate confusion matrix:
(confusion = table(Observed=Y, Predicted=yhat))
1 - mean(Y==yhat)
# Load the glmnet package
library(glmnet)
## Choose grid of values for the tuning parameter
grid = 10^seq(-4, 2, length.out=100)
## Fit a model with ridge penalty for each value of the tuning parameter
ridge_fit = glmnet(X1, Y, family="binomial", alpha=0, standardize=FALSE, lambda=grid)
# Examine the effect of the tuning parameter on the parameter estimates
plot(ridge_fit, xvar="lambda", col=1:10, label=TRUE)
# running cros validation to get lambda
ridge_cv_fit = cv.glmnet(X1, Y, family="binomial", alpha=0, standardize=FALSE, lambda=grid,
type.measure="class")
plot(ridge_cv_fit)
# Identify the optimal value for the tuning parameter
(lambda_ridge_min = ridge_cv_fit$lambda.min)
which_ridge_lasso = which(ridge_cv_fit$lambda == lambda_ridge_min)
## Find the parameter estimates associated with optimal value of the tuning parameter
coef(ridge_fit, s=lambda_ridge_min)
# Compute predicted probabilities:
phat = predict(ridge_fit, X1, s=lambda_ridge_min, type="response")
## Compute fitted values:
yhat = ifelse(phat > 0.5, 1, 0)
## Calculate confusion matrix:
(confusion = table(Observed=Y, Predicted=yhat))
1 - mean(Y==yhat)
# Load the MASS package
library(MASS)
# Sample indices of training data:
train_set = sample(c(TRUE, FALSE), nrow(breastcancer_lr_data), replace=TRUE)
# Perform LDA:
(lda_train=lda(Y~., data=breastcancer_lr_data[train_set,]))
library(klaR)
library(MASS)
#par(mar=c(1,1,1,1))
partimat(Y ~ ., data = breastcancer_lr_data, method = "lda")
# Compute fitted values for the validation data:
lda_test = predict(lda_train, breastcancer_lr_data[!train_set,])
yhat_test = lda_test$class
1 - mean(breastcancer_lr_data$Y[!train_set] == yhat_test)
# Load the nclSLR package
library(nclSLR)
library(MASS)
## Perform QDA on the training data:
(qda_train = qda(Y~., data=breastcancer_lr_data[train_set,]))
# Compute fitted values for the validation data:
qda_test = predict(qda_train, breastcancer_lr_data[!train_set,])
yhat_test = qda_test$class
## Compute test error:
1 - mean(breastcancer_lr_data$Y[!train_set] == yhat_test)
# Set the seed to make the analysis reproducible
set.seed(1)
# 10-fold cross validation
nfolds = 10
## Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
## Print first few fold-assignments
head(fold_index)
#Function to estimate the test error:
# inputting the predictor and dependent variables, folds index,  and methos which asks if it's lasso, lda or bic
reg_cv = function(X1, Y, fold_ind,method) {
Xy = data.frame(X1, Y=Y) # combining the predictor and output
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds) # creating a numeric vector to store error
for(fold in 1:nfolds) { # loooping over each fold
if (method=="BIC") { #running model and predicting for BIC
#using only the 5 variables selected from best subset selection models
tmp_fit = glm(Y ~ Cl.thickness+Marg.adhesion+Bare.nuclei+Bl.cromatin+Normal.nucleoli, data=Xy[fold_ind!=fold,], family="binomial")
phat = predict(tmp_fit, Xy[fold_ind==fold,],
type="response")
## Compute fitted values:
yhat = ifelse(phat > 0.5, 1, 0)
}else if (method=="lasso") { # model for lasso
#using the lambda values obtained in the model
tmp_fit = glmnet(Xy[fold_ind!=fold,-10], Xy[fold_ind!=fold,]$Y, family="binomial", alpha=1, standardize=FALSE, lambda=lambda_lasso_min)
phat = predict(tmp_fit, X1[fold_ind==fold,],
type="response")
## Compute fitted  values:
yhat = ifelse(phat > 0.5, 1, 0)
} else { # model for LDA
tmp_fit=lda(Y~., data=Xy[fold_ind!=fold,]) # training
lda_test = predict(tmp_fit, Xy[fold_ind==fold,]) # predicting
yhat=lda_test$class
}
# finding the confusion matrix
(confusion = table(Observed=Xy[fold_ind==fold,]$Y, Predicted=yhat))
# storing the error
cv_errors[fold] = 1 - mean(Xy[fold_ind==fold,]$Y==yhat)
}
# taking the average of error across the folds
fold_sizes = numeric(nfolds)
for(fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
# calling the function for BIC
reg_cv(X1,Y,fold_index,method = "BIC")
# calling the function for lasso
reg_cv(X1,Y,fold_index,method = "lasso")
# calling the function for lda
reg_cv(X1,Y,fold_index,method = "LDA")
??read.sps
library(foreign)
ff<- read.spss("D:/MSc DS/Dessertatin/Behaviour Data/8128spss_BCBFC942835A6A827A7A0368519B6CAAD2049EFD940BF31A1A226FE01630B89E_V1/UKDA-8128-spss/spss/spss19/uktus15_diary_ep_long.sav")
View(ff)
dd<- data.frame(ff)
View(dd)
# CSC8639 - Project and Dissertation in Data Science
library(ProjectTemplate)
setwd("D:/MSc DS/CSC8639  Project and Dissertation in Data Science//")
setwd("NICA_Behavioral_Analytics/reports/")
library(rticles)
library(TinyTex )
library(TinyTex)
